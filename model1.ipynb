{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Force Use GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)} is available\")\n",
    "else:\n",
    "    print('\\nNo GPU available')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load preprocessed data and stores them in pickle files. Images size after preprocessing is 128 x 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_folder = \"pickle/\"\n",
    "\n",
    "# Load pickled datasets\n",
    "with open(pickle_folder + \"X_train.pickle\", \"rb\") as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(pickle_folder + \"Y_train.pickle\", \"rb\") as f:\n",
    "    Y_train = pickle.load(f)\n",
    "\n",
    "with open(pickle_folder + \"X_test.pickle\", \"rb\") as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(pickle_folder + \"Y_test.pickle\", \"rb\") as f:\n",
    "    Y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converts the training and testing images into a PyTorch tensor. unsqueeze(1) changes the shape from [128, 128] to [1, 128, 128] to account for 1 channel in the grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float().unsqueeze(1)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test).float().unsqueeze(1)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span syle=\"color:blue\">Model</span>\n",
    "- Create the Neural Network with 5 convolutional layers, 1 maxpool, and 4 fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=512, kernel_size=2, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=2, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=2, stride=2, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (torch.relu(self.conv1(x)))\n",
    "        x = self.maxpool(torch.relu(self.conv2(x)))\n",
    "        x = (torch.relu(self.conv3(x)))\n",
    "        x = self.maxpool(torch.relu(self.conv4(x)))\n",
    "        x = (torch.relu(self.conv5(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataLoader is used to break the dataset up into batches. By setting shuffle=True the data is shuffled at the beggining of each epoch. \n",
    "- Cross entropy loss and stochastic gradient descent are chosen as hyperparamaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloaders\n",
    "train_dataset = TensorDataset(X_train, Y_train.float())\n",
    "test_dataset = TensorDataset(X_test, Y_test.float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = NeuralNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy calculation function\n",
    "def calculate_metrics(y_pred, y_true):\n",
    "    # Convert predictions to class indices (max value)\n",
    "    predicted = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    # Compute confusion matrix components\n",
    "    tp = ((predicted == 1) & (y_true == 1)).sum().item()  # True Positive\n",
    "    tn = ((predicted == 0) & (y_true == 0)).sum().item()  # True Negative\n",
    "    fp = ((predicted == 1) & (y_true == 0)).sum().item()  # False Positive\n",
    "    fn = ((predicted == 0) & (y_true == 1)).sum().item()  # False Negative\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return fp, tp, fn, tn, accuracy, precision, recall, specificity, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with detailed metrics tracked after each epoch\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize metrics for the epoch\n",
    "    train_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'specificity': 0, 'f1': 0}\n",
    "    test_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'specificity': 0, 'f1': 0}\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics for the current batch\n",
    "        fp, tp, fn, tn, accuracy, precision, recall, specificity, f1 = calculate_metrics(outputs, batch_Y)\n",
    "        train_metrics['accuracy'] += accuracy\n",
    "        train_metrics['precision'] += precision\n",
    "        train_metrics['recall'] += recall\n",
    "        train_metrics['specificity'] += specificity\n",
    "        train_metrics['f1'] += f1\n",
    "\n",
    "    # Average metrics over the training set\n",
    "    epoch_loss /= len(train_loader)\n",
    "    train_metrics = {k: v / len(train_loader) for k, v in train_metrics.items()}\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "writer = SummaryWriter()\n",
    "            # Calculate metrics for the current batch\n",
    "            fp, tp, fn, tn, accuracy, precision, recall, specificity, f1 = calculate_metrics(outputs, batch_Y)\n",
    "            test_metrics['accuracy'] += accuracy\n",
    "            test_metrics['precision'] += precision\n",
    "            test_metrics['recall'] += recall\n",
    "            test_metrics['specificity'] += specificity\n",
    "            test_metrics['f1'] += f1\n",
    "\n",
    "    # Average metrics over the test set\n",
    "    test_metrics = {k: v / len(test_loader) for k, v in test_metrics.items()}\n",
    "\n",
    "    # Log metrics to TensorBoard for training\n",
    "    writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
    "    for metric_name, metric_value in train_metrics.items():\n",
    "        writer.add_scalar(f\"Train/{metric_name.capitalize()}\", metric_value, epoch)\n",
    "\n",
    "    # Log metrics to TensorBoard for testing\n",
    "    for metric_name, metric_value in test_metrics.items():\n",
    "        writer.add_scalar(f\"Test/{metric_name.capitalize()}\", metric_value, epoch)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Loss: {epoch_loss:.4f}, '\n",
    "          f'Train Accuracy: {train_metrics[\"accuracy\"]*100:.2f}%, '\n",
    "          f'Test Accuracy: {test_metrics[\"accuracy\"]*100:.2f}%, '\n",
    "          f'Train Recall: {train_metrics[\"recall\"]:.4f}, '\n",
    "          f'Test Recall: {test_metrics[\"recall\"]:.4f}, '\n",
    "          f'Train F1 Score: {train_metrics[\"f1\"]:.4f}, '\n",
    "          f'Test F1 Score: {test_metrics[\"f1\"]:.4f}')\n",
    "\n",
    "# Close the writer after training is complete\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-tumor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
